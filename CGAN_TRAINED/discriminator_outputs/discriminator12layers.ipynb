{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0becdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/212 [00:01<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 1, 1, 1])\n",
      "torch.Size([8, 1, 1, 1, 1])\n",
      "torch.Size([8, 1, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/212 [01:32<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-1-b55646210287>\", line 123, in forward\n    x = layer(x)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/container.py\", line 204, in forward\n    input = module(input)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/conv.py\", line 613, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/conv.py\", line 608, in _conv_forward\n    return F.conv3d(\nRuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b55646210287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# Fake samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mfake_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_volumes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mfake_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"<ipython-input-1-b55646210287>\", line 123, in forward\n    x = layer(x)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/container.py\", line 204, in forward\n    input = module(input)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/conv.py\", line 613, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.8/site-packages/torch/nn/modules/conv.py\", line 608, in _conv_forward\n    return F.conv3d(\nRuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset Class\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_folder, transform=None):\n",
    "        self.csv = pd.read_csv(csv_file)\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        self.pairs = self.csv.groupby('uid')  \n",
    "        self.valid_uids = self.check_integrity()\n",
    "\n",
    "    def check_integrity(self):\n",
    "        valid_uids = []\n",
    "        for uid, group in self.pairs:\n",
    "            frontal_data = group[group['projection'] == 'Frontal']\n",
    "            lateral_data = group[group['projection'] == 'Lateral']\n",
    "            if frontal_data.empty or lateral_data.empty:\n",
    "                continue\n",
    "            valid_uids.append(uid)\n",
    "        return valid_uids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_uids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        group_key = self.valid_uids[idx]\n",
    "        uid = self.pairs.get_group(group_key)\n",
    "\n",
    "        frontal_img_path = os.path.join(self.img_folder, uid[uid['projection'] == 'Frontal']['filename'].values[0])\n",
    "        lateral_img_path = os.path.join(self.img_folder, uid[uid['projection'] == 'Lateral']['filename'].values[0])\n",
    "        frontal_img = Image.open(frontal_img_path).convert(\"RGB\")\n",
    "        lateral_img = Image.open(lateral_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            frontal_img = self.transform(frontal_img)\n",
    "            lateral_img = self.transform(lateral_img)\n",
    "\n",
    "        return frontal_img, lateral_img\n",
    "\n",
    "# Define the Dense Block\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels + i * growth_rate, growth_rate, kernel_size=3, padding=1),\n",
    "                nn.InstanceNorm2d(growth_rate),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            new_features = layer(x)\n",
    "            x = torch.cat([x, new_features], dim=1)\n",
    "        return x\n",
    "\n",
    "# Define the Basic3D Block\n",
    "class Basic3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Basic3D, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# Define 2D to 3D Connection (Connection-C)\n",
    "class Connection2Dto3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Connection2Dto3D, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv3d = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        x = x.unsqueeze(2)\n",
    "        x = self.conv3d(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Updated Discriminator to be self-adjusting\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, base_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv3d(in_channels, base_channels, kernel_size=2, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add subsequent convolutional layers\n",
    "        num_layers = 6  # Number of convolutional layers\n",
    "        for i in range(1, num_layers):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv3d(base_channels * (2 ** (i - 1)), base_channels * (2 ** i), kernel_size=2, stride=2, padding=1),\n",
    "                    nn.InstanceNorm3d(base_channels * (2 ** i)),\n",
    "                    nn.LeakyReLU(0.2, inplace=True)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Final layer\n",
    "        self.final_layer = nn.Conv3d(base_channels * (2 ** (num_layers - 1)), 1, kernel_size=2, stride=1, padding=0)\n",
    "        self.pooling = nn.AdaptiveAvgPool3d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Ensure final output is flattened\n",
    "        x = self.final_layer(x)\n",
    "        x = self.pooling(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "# Updated Generator to verify and propagate image size\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_dense_layers, out_channels, image_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_upconv_layers = int(torch.log2(torch.tensor(image_size // 4)).item())\n",
    "\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1),\n",
    "            DenseBlock(growth_rate, growth_rate, num_dense_layers),\n",
    "            nn.Conv2d(growth_rate * (num_dense_layers + 1), growth_rate, kernel_size=3, stride=2, padding=1)  # Compression\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1),\n",
    "            DenseBlock(growth_rate, growth_rate, num_dense_layers),\n",
    "            nn.Conv2d(growth_rate * (num_dense_layers + 1), growth_rate, kernel_size=3, stride=2, padding=1)  # Compression\n",
    "        )\n",
    "\n",
    "        self.connection_a = Connection2Dto3D(growth_rate, growth_rate)\n",
    "        self.connection_b = nn.Conv3d(growth_rate * 2, growth_rate, kernel_size=3, padding=1)\n",
    "        self.connection_c = nn.Conv3d(growth_rate, growth_rate, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv_layers = nn.ModuleList()\n",
    "        for _ in range(self.num_upconv_layers):\n",
    "            self.upconv_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose3d(growth_rate, growth_rate, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm3d(growth_rate),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.final_layer = nn.Conv3d(growth_rate, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x1 = self.encoder1(x1)\n",
    "        x2 = self.encoder2(x2)\n",
    "        x1 = self.connection_a(x1)\n",
    "        x2 = self.connection_a(x2)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = self.connection_b(x)\n",
    "        for upconv in self.upconv_layers:\n",
    "            x = upconv(x)\n",
    "\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Paths and Hyperparameters\n",
    "    IMAGE_FOLDER = '/projectnb/ec523kb/projects/teams_Fall_2024/Team_11/Adwait/Work_on_this_code/images/images_normalized/'\n",
    "    CSV_FILE = '/projectnb/ec523kb/projects/teams_Fall_2024/Team_11/Adwait/Work_on_this_code/indiana_projections.csv'\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 10\n",
    "    IMAGE_SIZE = 120\n",
    "    LATENT_DIM = 540\n",
    "    IN_CHANNELS = 3\n",
    "    OUT_CHANNELS = 1\n",
    "    GROWTH_RATE = 16\n",
    "    NUM_DENSE_LAYERS = 4\n",
    "    BASE_CHANNELS = 64\n",
    "\n",
    "    d_real_losses = []\n",
    "    d_fake_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    # Dataset and Dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = XRayDataset(CSV_FILE, IMAGE_FOLDER, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Initialize Generator and Discriminator\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator = Generator(\n",
    "        in_channels=IN_CHANNELS,\n",
    "        growth_rate=GROWTH_RATE,\n",
    "        num_dense_layers=NUM_DENSE_LAYERS,\n",
    "        out_channels=OUT_CHANNELS,\n",
    "        image_size=IMAGE_SIZE\n",
    "    ).to(device)\n",
    "    generator = nn.DataParallel(generator)\n",
    "\n",
    "    discriminator = Discriminator(OUT_CHANNELS, BASE_CHANNELS).to(device)\n",
    "    discriminator = nn.DataParallel(discriminator)\n",
    "\n",
    "    g_optim = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optim = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    g_criterion = nn.MSELoss()\n",
    "    d_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        g_epoch_loss = 0\n",
    "        d_epoch_loss = 0\n",
    "\n",
    "        with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "            for frontal_img, lateral_img in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "                frontal_img = frontal_img.to(device)\n",
    "                lateral_img = lateral_img.to(device)\n",
    "                real_labels = torch.ones((frontal_img.size(0),), device=device)\n",
    "                fake_labels = torch.zeros((frontal_img.size(0),), device=device)\n",
    "                # ===================== Train Discriminator =====================\n",
    "                # Generate fake samples\n",
    "                fake_volumes = generator(frontal_img, lateral_img)\n",
    "\n",
    "                real_outputs = discriminator(fake_volumes.detach())\n",
    "                real_loss = d_criterion(real_outputs, real_labels)\n",
    "\n",
    "                # Fake samples\n",
    "                fake_outputs = discriminator(fake_volumes.detach())\n",
    "                fake_loss = d_criterion(fake_outputs, fake_labels)\n",
    "\n",
    "                # Backpropagation for Discriminator\n",
    "                d_loss = real_loss + fake_loss\n",
    "                d_optim.zero_grad()\n",
    "                d_loss.backward()\n",
    "                d_optim.step()\n",
    "                d_real_losses.append(real_loss.item())\n",
    "                d_fake_losses.append(fake_loss.item())\n",
    "                d_epoch_loss += d_loss.item()\n",
    "\n",
    "                # ===================== Train Generator =====================\n",
    "                fake_outputs = discriminator(fake_volumes)\n",
    "                g_loss = g_criterion(fake_outputs, real_labels)\n",
    "\n",
    "                g_optim.zero_grad()\n",
    "                g_loss.backward()\n",
    "                g_optim.step()\n",
    "\n",
    "                g_epoch_loss += g_loss.item()\n",
    "                g_losses.append(g_loss.item())\n",
    "\n",
    "                # Ground truths\n",
    "\n",
    "\n",
    "                tepoch.set_postfix(d_loss=d_loss.item(), g_loss=g_loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Generator Loss: {g_epoch_loss:.4f}, Discriminator Loss: {d_epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # Save Models\n",
    "    torch.save(generator.state_dict(), f\"generator_{BATCH_SIZE}_{IMAGE_SIZE}_epoch_{EPOCHS}.pth\")\n",
    "    torch.save(discriminator.state_dict(), f\"discriminator_{BATCH_SIZE}_{IMAGE_SIZE}_epoch_{EPOCHS}.pth\")\n",
    "\n",
    "    print(f\"Models saved at epoch {EPOCHS} with batch size {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(d_real_losses, label='Discriminator Real Loss', color='r')\n",
    "    plt.plot(d_fake_losses, label='Discriminator Fake Loss', color='orange')\n",
    "    plt.plot(g_losses, label='Generator Loss', color='b')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves during Training')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'losses_epoch_{EPOCHS}_batch_{BATCH_SIZE}_image_{IMAGE_SIZE}_3DGANDiscriminator.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495910c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
